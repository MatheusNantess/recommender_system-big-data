{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Feature Engineering - Checkins (Vers√£o Spark)\n",
    "\n",
    "**Convers√£o do notebook pandas ‚Üí Spark**\n",
    "\n",
    "Este notebook:\n",
    "1. L√™ checkins da camada Bronze (Spark)\n",
    "2. Processa e agrega por business_id\n",
    "3. Faz join com Business e Tips da Silver\n",
    "4. Aplica normaliza√ß√£o (log + MinMaxScaler)\n",
    "5. Salva features finais na Gold layer\n",
    "\n",
    "**Vantagens vs Pandas:**\n",
    "- ‚úÖ Sem problemas de mem√≥ria\n",
    "- ‚úÖ Processamento paralelo\n",
    "- ‚úÖ Acessa datalake diretamente\n",
    "- ‚úÖ Escal√°vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, split, explode, size, log1p, \n",
    "    when, coalesce, lit, broadcast\n",
    ")\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spark_session",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° CONFIGURA√á√ÉO SPARK OTIMIZADA\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Checkin Feature Engineering\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü•â Bronze: /home/jovyan/work/data/bronze\n",
      "ü•à Silver: /home/jovyan/work/data/silver\n",
      "ü•á Gold: /home/jovyan/work/data/gold\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√£o de Paths\n",
    "BASE_PATH = '/home/jovyan/work'\n",
    "DATA_PATH = f'{BASE_PATH}/data'\n",
    "BRONZE_PATH = f'{DATA_PATH}/bronze'\n",
    "SILVER_PATH = f'{DATA_PATH}/silver'\n",
    "GOLD_PATH = f'{DATA_PATH}/gold'\n",
    "\n",
    "print(f\"ü•â Bronze: {BRONZE_PATH}\")\n",
    "print(f\"ü•à Silver: {SILVER_PATH}\")\n",
    "print(f\"ü•á Gold: {GOLD_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_1",
   "metadata": {},
   "source": [
    "## 1. Carregar Dados do Datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• [1/5] Carregando dados do datalake...\n",
      "\n",
      "   üì¶ Carregando checkins...\n",
      "      ‚úÖ Checkins: 131,930 registros\n",
      "   üì¶ Carregando business...\n",
      "      ‚úÖ Business: 64,645 registros\n",
      "   üì¶ Carregando tips...\n",
      "      ‚úÖ Tips: 44,904 registros\n",
      "\n",
      "‚úÖ Dados carregados!\n"
     ]
    }
   ],
   "source": [
    "print(\"üì• [1/5] Carregando dados do datalake...\\n\")\n",
    "\n",
    "# 1. Checkin (Bronze)\n",
    "print(\"   üì¶ Carregando checkins...\")\n",
    "df_checkin = spark.read.parquet(f\"{BRONZE_PATH}/checkin\")\n",
    "print(f\"      ‚úÖ Checkins: {df_checkin.count():,} registros\")\n",
    "\n",
    "# 2. Business (Silver) - J√° filtrado\n",
    "print(\"   üì¶ Carregando business...\")\n",
    "df_business = spark.read.parquet(f\"{SILVER_PATH}/business\")\n",
    "print(f\"      ‚úÖ Business: {df_business.count():,} registros\")\n",
    "\n",
    "print(\"   üì¶ Carregando tips...\")\n",
    "try:\n",
    "    df_tips = spark.read.parquet(f\"{SILVER_PATH}/tip_features_business\")\n",
    "    print(f\"      ‚úÖ Tips: {df_tips.count():,} registros\")\n",
    "    has_tips = True\n",
    "except:\n",
    "    print(\"      ‚ö†Ô∏è Tips n√£o encontrado, criando features vazias\")\n",
    "    has_tips = False\n",
    "\n",
    "print(\"\\n‚úÖ Dados carregados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "show_schema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Schema do Checkin:\n",
      "\n",
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n",
      "\n",
      "üìä Amostra:\n",
      "+----------------------+--------------------------------------------------+\n",
      "|           business_id|                                              date|\n",
      "+----------------------+--------------------------------------------------+\n",
      "|9D0dAIUpSoC91NKHK5OAPQ|2015-02-13 22:42:58, 2015-04-24 20:45:29, 2015-...|\n",
      "|9D1Vuxuh-hvwkYBrqje1nA|2010-07-25 22:42:21, 2010-08-22 16:26:12, 2010-...|\n",
      "|9D3mvHmxQouaOcizFKg1dg|2011-07-24 01:09:08, 2011-10-04 15:41:08, 2012-...|\n",
      "|9D4gd_k6s8xzeWKXrqYcmA|2012-01-30 00:55:35, 2013-02-02 13:58:34, 2013-...|\n",
      "|9D6YY6_J1diGiBOQJJ7juA|2014-02-24 18:49:29, 2015-04-28 20:01:29, 2021-...|\n",
      "+----------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar schema dos checkins\n",
    "print(\"üìã Schema do Checkin:\\n\")\n",
    "df_checkin.printSchema()\n",
    "\n",
    "print(\"\\nüìä Amostra:\")\n",
    "df_checkin.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_2",
   "metadata": {},
   "source": [
    "## 2. Processar Checkins\n",
    "\n",
    "O campo `date` √© uma string com m√∫ltiplas datas separadas por v√≠rgula.  \n",
    "Precisamos contar quantas datas existem para cada business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "process_checkin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß [2/5] Processando checkins...\n",
      "\n",
      "   ‚úÖ Filtrados para business v√°lidos: 56,107\n",
      "   ‚úÖ Checkins agregados: 56,107 businesses\n",
      "\n",
      "   üìä Estat√≠sticas de checkins:\n",
      "+-------+------------------+\n",
      "|summary|     checkin_total|\n",
      "+-------+------------------+\n",
      "|  count|             56107|\n",
      "|   mean|139.09768834548274|\n",
      "| stddev| 579.6346304425077|\n",
      "|    min|                 1|\n",
      "|    25%|                 6|\n",
      "|    50%|                22|\n",
      "|    75%|                89|\n",
      "|    max|             52144|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîß [2/5] Processando checkins...\\n\")\n",
    "\n",
    "# Estrat√©gia: Contar quantas v√≠rgulas tem na string + 1\n",
    "# Ex: \"2020-01-01, 2020-01-02\" ‚Üí 2 checkins\n",
    "\n",
    "# Filtrar apenas business_ids v√°lidos (que est√£o na Silver)\n",
    "valid_business_ids = df_business.select('business_id')\n",
    "\n",
    "df_checkin_filtered = df_checkin.join(\n",
    "    broadcast(valid_business_ids),\n",
    "    on='business_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Filtrados para business v√°lidos: {df_checkin_filtered.count():,}\")\n",
    "\n",
    "# Contar checkins: split por v√≠rgula e pegar tamanho\n",
    "df_checkin_count = df_checkin_filtered \\\n",
    "    .withColumn('checkin_total', size(split(col('date'), ',')))\n",
    "\n",
    "# Agregar por business_id\n",
    "df_checkin_agg = df_checkin_count \\\n",
    "    .groupBy('business_id') \\\n",
    "    .agg(\n",
    "        F.sum('checkin_total').alias('checkin_total')\n",
    "    )\n",
    "\n",
    "print(f\"   ‚úÖ Checkins agregados: {df_checkin_agg.count():,} businesses\")\n",
    "\n",
    "# Estat√≠sticas\n",
    "print(\"\\n   üìä Estat√≠sticas de checkins:\")\n",
    "df_checkin_agg.select('checkin_total').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_3",
   "metadata": {},
   "source": [
    "## 3. Join com Business e Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "merge_all",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó [3/5] Unificando tabelas...\n",
      "\n",
      "   ‚úÖ Checkins integrados\n",
      "   ‚úÖ Tips integrados\n",
      "\n",
      "   üìä Total de registros: 64,645\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîó [3/5] Unificando tabelas...\\n\")\n",
    "\n",
    "# A. Come√ßar com Business (base)\n",
    "df_final = df_business\n",
    "\n",
    "# B. Left Join com Checkins\n",
    "df_final = df_final.join(\n",
    "    df_checkin_agg,\n",
    "    on='business_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Preencher nulos com 0 (businesses sem checkin)\n",
    "df_final = df_final.fillna({'checkin_total': 0})\n",
    "\n",
    "print(f\"   ‚úÖ Checkins integrados\")\n",
    "\n",
    "# C. Left Join com Tips (se existir)\n",
    "if has_tips:\n",
    "    df_final = df_final.join(\n",
    "        df_tips,\n",
    "        on='business_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Preencher nulos\n",
    "    df_final = df_final.fillna({\n",
    "        'tip_count_log': 0,\n",
    "        'recency_score': 0\n",
    "    })\n",
    "    \n",
    "    print(f\"   ‚úÖ Tips integrados\")\n",
    "else:\n",
    "    # Criar colunas de tips com zeros\n",
    "    df_final = df_final \\\n",
    "        .withColumn('tip_count_log', lit(0.0)) \\\n",
    "        .withColumn('recency_score', lit(0.0))\n",
    "    \n",
    "    print(f\"   ‚úÖ Colunas de tips criadas (zeros)\")\n",
    "\n",
    "print(f\"\\n   üìä Total de registros: {df_final.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efaf855-d3fe-4300-b10c-5aba6ecf7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_4",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering - Normaliza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ [4/5] Aplicando transforma√ß√µes...\n",
      "\n",
      "   üìê Aplicando log1p no checkin_total...\n",
      "   üìê Normalizando stars (1-5 ‚Üí 0-1)...\n",
      "\n",
      "   ‚úÖ Transforma√ß√µes aplicadas!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüß™ [4/5] Aplicando transforma√ß√µes...\\n\")\n",
    "\n",
    "# A. Log transformation no checkin_total\n",
    "print(\"   üìê Aplicando log1p no checkin_total...\")\n",
    "df_final = df_final.withColumn(\n",
    "    'checkin_total_log',\n",
    "    log1p(col('checkin_total'))\n",
    ")\n",
    "\n",
    "# B. Normalizar review_count (se existir)\n",
    "if 'review_count' in df_final.columns:\n",
    "    print(\"   üìê Aplicando log1p no review_count...\")\n",
    "    df_final = df_final.withColumn(\n",
    "        'review_count_log',\n",
    "        log1p(col('review_count'))\n",
    "    )\n",
    "\n",
    "# C. Normalizar stars (se existir) - de 1-5 para 0-1\n",
    "if 'stars' in df_final.columns:\n",
    "    print(\"   üìê Normalizando stars (1-5 ‚Üí 0-1)...\")\n",
    "    df_final = df_final.withColumn(\n",
    "        'stars',\n",
    "        (col('stars') - 1) / 4  # Min=1, Max=5 ‚Üí (x-1)/4\n",
    "    )\n",
    "\n",
    "print(\"\\n   ‚úÖ Transforma√ß√µes aplicadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minmax_normalization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Aplicando MinMaxScaler...\n",
      "\n",
      "   ‚öôÔ∏è  Normalizando checkin_total_log...\n",
      "\n",
      "   ‚úÖ MinMaxScaler aplicado!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä Aplicando MinMaxScaler...\\n\")\n",
    "\n",
    "# Colunas para normalizar\n",
    "cols_to_normalize = ['checkin_total_log']\n",
    "\n",
    "if 'review_count_log' in df_final.columns:\n",
    "    cols_to_normalize.append('review_count_log')\n",
    "\n",
    "# Aplicar MinMaxScaler\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "for col_name in cols_to_normalize:\n",
    "    print(f\"   ‚öôÔ∏è  Normalizando {col_name}...\")\n",
    "    \n",
    "    # 1. Criar Vector (MinMaxScaler precisa de Vector)\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[col_name],\n",
    "        outputCol=f\"{col_name}_vec\"\n",
    "    )\n",
    "    df_final = assembler.transform(df_final)\n",
    "    \n",
    "    # 2. Aplicar MinMaxScaler\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=f\"{col_name}_vec\",\n",
    "        outputCol=f\"{col_name}_scaled\"\n",
    "    )\n",
    "    scaler_model = scaler.fit(df_final)\n",
    "    df_final = scaler_model.transform(df_final)\n",
    "    \n",
    "    # 3. Extrair valor do Vector de volta para coluna simples\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.ml.linalg import VectorUDT, DenseVector\n",
    "    \n",
    "    def extract_value(vector):\n",
    "        if vector is None:\n",
    "            return 0.0\n",
    "        return float(vector[0])\n",
    "    \n",
    "    extract_udf = udf(extract_value, DoubleType())\n",
    "    \n",
    "    df_final = df_final.withColumn(\n",
    "        col_name,\n",
    "        extract_udf(col(f\"{col_name}_scaled\"))\n",
    "    )\n",
    "    \n",
    "    # 4. Dropar colunas tempor√°rias\n",
    "    df_final = df_final.drop(f\"{col_name}_vec\", f\"{col_name}_scaled\")\n",
    "\n",
    "print(\"\\n   ‚úÖ MinMaxScaler aplicado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a9e132c-b120-4667-9d7d-ce7a1d65163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------\n",
      " business_id       | qWH526XL2vBmrOKCNcVthw                                                                                                         \n",
      " name              | Green Hills Pharmacy                                                                                                           \n",
      " city              | Nashville                                                                                                                      \n",
      " categories        | Vocational & Technical School, Specialty Schools, Adult Education, Education, Drugstores, Health & Medical, Pharmacy, Shopping \n",
      " stars             | 0.875                                                                                                                          \n",
      " is_open           | 1                                                                                                                              \n",
      " log_review_count  | 2.5649493574615367                                                                                                             \n",
      " checkin_total     | 1                                                                                                                              \n",
      " tip_count_log     | 0.05663480599756732                                                                                                            \n",
      " recency_score     | 0.47171626283519685                                                                                                            \n",
      " checkin_total_log | 0.0638152266197278                                                                                                             \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resultado\n",
    "print(\"\\nüìä Features finais:\\n\")\n",
    "\n",
    "cols_to_show = ['business_id', 'name', 'city', 'stars', \n",
    "                'checkin_total', 'checkin_total_log', \n",
    "                'tip_count_log', 'recency_score']\n",
    "\n",
    "# Adicionar review_count_log se existir\n",
    "if 'review_count_log' in df_final.columns:\n",
    "    cols_to_show.append('review_count_log')\n",
    "\n",
    "# Filtrar apenas colunas que existem\n",
    "cols_to_show = [c for c in cols_to_show if c in df_final.columns]\n",
    "\n",
    "df_final.select(cols_to_show).show(10, truncate=30)\n",
    "\n",
    "# Estat√≠sticas das features normalizadas\n",
    "print(\"\\nüìà Estat√≠sticas das features normalizadas:\")\n",
    "df_final.select('checkin_total_log', 'tip_count_log', 'recency_score').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_5",
   "metadata": {},
   "source": [
    "## 5. Salvar na Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "save_gold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ [5/5] Salvando na Silver Layer...\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ ARQUIVO FINAL GERADO: /home/jovyan/work/data/silver/item_features_enriched\n",
      "   üìä Dimens√µes: 64,645 linhas x 11 colunas\n",
      "   üì¶ Parti√ß√µes: 10\n",
      "   üóúÔ∏è  Compress√£o: SNAPPY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüíæ [5/5] Salvando na Silver Layer...\\n\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Criar diret√≥rio Silver se n√£o existir\n",
    "os.makedirs(SILVER_PATH, exist_ok=True)\n",
    "\n",
    "output_path = f'{SILVER_PATH}/item_features_enriched'\n",
    "\n",
    "# Remover se existir\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "    print(f\"   üóëÔ∏è  Removido arquivo antigo: {output_path}\")\n",
    "\n",
    "# Salvar\n",
    "df_final \\\n",
    "    .repartition(10) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .option('compression', 'snappy') \\\n",
    "    .parquet(output_path)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ ARQUIVO FINAL GERADO: {output_path}\")\n",
    "print(f\"   üìä Dimens√µes: {df_final.count():,} linhas x {len(df_final.columns)} colunas\")\n",
    "print(f\"   üì¶ Parti√ß√µes: 10\")\n",
    "print(f\"   üóúÔ∏è  Compress√£o: SNAPPY\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "verify_output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Verificando arquivo salvo...\n",
      "\n",
      "‚úÖ Arquivo lido com sucesso!\n",
      "   Total de registros: 64,645\n",
      "\n",
      "üìã Colunas:\n",
      "   - business_id\n",
      "   - name\n",
      "   - city\n",
      "   - categories\n",
      "   - stars\n",
      "   - is_open\n",
      "   - log_review_count\n",
      "   - checkin_total\n",
      "   - tip_count_log\n",
      "   - recency_score\n",
      "   - checkin_total_log\n",
      "\n",
      "üìä Amostra dos dados:\n",
      "+-----------+----------+----------+----------+-----+-------+----------------+-------------+-------------+-------------+-----------------+\n",
      "|business_id|      name|      city|categories|stars|is_open|log_review_count|checkin_total|tip_count_log|recency_score|checkin_total_log|\n",
      "+-----------+----------+----------+----------+-----+-------+----------------+-------------+-------------+-------------+-----------------+\n",
      "| 0OVkfxm...|AMC DIN...|Philade...|Cinema,...|0.625|      1|      3.68887...|           43|   0.09681...|   0.54508...|       0.34839...|\n",
      "| 7RYxPJo...|Peach P...|Philade...|Event P...|  1.0|      1|      2.19722...|            0|          0.0|          0.0|              0.0|\n",
      "| Y-oRnYf...|Buck's ...|Indiana...|Grocery...|0.875|      1|      1.94591...|          118|          0.0|          0.0|       0.43999...|\n",
      "| fJhOwBl...|Home2 S...|Philade...|Hotels ...|0.625|      1|      4.97673...|          382|   0.27180...|   0.40139...|       0.54761...|\n",
      "| D7eybff...|Blush &...|      Reno|Massage...|0.875|      1|      2.63905...|            2|          0.0|          0.0|       0.10114...|\n",
      "+-----------+----------+----------+----------+-----+-------+----------------+-------------+-------------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar arquivo salvo\n",
    "print(\"\\nüîç Verificando arquivo salvo...\\n\")\n",
    "\n",
    "df_verify = spark.read.parquet(output_path)\n",
    "\n",
    "print(f\"‚úÖ Arquivo lido com sucesso!\")\n",
    "print(f\"   Total de registros: {df_verify.count():,}\")\n",
    "print(f\"\\nüìã Colunas:\")\n",
    "for col in df_verify.columns:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(\"\\nüìä Amostra dos dados:\")\n",
    "df_verify.show(5, truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza\n",
    "print(\"\\nüßπ Limpando cache...\")\n",
    "spark.catalog.clearCache()\n",
    "print(\"‚úÖ Cache limpo!\")\n",
    "\n",
    "print(\"\\nüéâ PROCESSAMENTO COMPLETO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Resumo do Pipeline\n",
    "\n",
    "**Input:**\n",
    "- `bronze/checkin` - Checkins brutos\n",
    "- `silver/business` - Business filtrados\n",
    "- `silver/tip_features_business` - Features de tips (opcional)\n",
    "\n",
    "**Transforma√ß√µes:**\n",
    "1. ‚úÖ Contagem de checkins por business\n",
    "2. ‚úÖ Join com business e tips\n",
    "3. ‚úÖ Log transformation (checkin_total, review_count)\n",
    "4. ‚úÖ Normaliza√ß√£o stars (1-5 ‚Üí 0-1)\n",
    "5. ‚úÖ MinMaxScaler em features num√©ricas\n",
    "\n",
    "**Output:**\n",
    "- `gold/item_features_enriched` - Features prontas para modelo\n",
    "\n",
    "**Features Finais:**\n",
    "- `business_id` - ID √∫nico\n",
    "- `name` - Nome do estabelecimento\n",
    "- `city` - Cidade\n",
    "- `categories` - Categorias\n",
    "- `stars` - Rating normalizado [0-1]\n",
    "- `checkin_total` - Total de checkins (raw)\n",
    "- `checkin_total_log` - Checkins normalizados [0-1]\n",
    "- `review_count_log` - Reviews normalizados [0-1]\n",
    "- `tip_count_log` - Tips normalizados [0-1]\n",
    "- `recency_score` - Score de rec√™ncia [0-1]\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
